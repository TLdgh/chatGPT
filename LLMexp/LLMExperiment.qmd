---
title: "LLMExperiment"
author: "Teng Li"
date: "2025-03-20"
format: 
  html:
    toc: true
    toc_depth: 2
    toc-expand: true
server: shiny
---

<style>
  /* Custom Styling for ANOVA Output */
  .anova-output {
    background-color: #f8f9fa;  /* Light gray background */
    border: 0.5px dashed #333;
    font-family: monospace;      /* Uses a monospace font for code-style look */
    font-size: 14px;             /* Adjust font size */
    white-space: pre-wrap;       /* Ensures text wraps properly */
    overflow-x: auto;            /* Enables horizontal scrolling if needed */
    color: #333;                 /* Dark text for readability */
  }
  .anova-title {
  font-weight: normal;        /* Explicitly set to not bold */
  font-size: 14px;           /* Adjust the size */
  color: #333;            /* Set title color */
}
</style>



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning = FALSE)
set.seed(100)
library(shiny)
library(plotly)
library(tidyverse)
library(R6)
library(here)
library(kableExtra)
library(htmltools)

source(here("LLMexp", "Codespace.R"))

exp_results=read.csv(here("results", "exp_results.csv"), header = TRUE)%>%select(-c(1:2))%>%
  mutate(ResponseAuth = cosine_sim_Auth==1.0, ResponseTitle = cosine_sim_Title==1.0)

countdata<-exp_results%>%
  transmute(Model=factor(Model),Primary_Cat=factor(Primary_Cat),ResponseAuth,ResponseTitle)%>%
  group_by(Model,Primary_Cat)%>%
  summarise(CountAuth=sum(ResponseAuth), CountTitle=sum(ResponseTitle), .groups = 'drop')%>%mutate(Total=200)

summ_results=read.csv(here("results", "exp_results_summ.csv"), header = TRUE)%>%as_tibble()
```

## Introduction

Intelligent Document Processing (IDP) is a crucial task in natural language processing (NLP), enabling automated extraction of structured information from unstructured documents. With the advancement of large language models (LLMs), their ability to accurately extract key metadata and summarize information from scientific documents remains an open question. In this study, we evaluate the performance of two state-of-the-art LLMs, Gemini 3.5 and Llama 3.1, in extracting titles and author names from research papers. In addition, we evaluate their ability of summarizing the paper content by examining the cosine similarity between the outcome and the abstract. We employ a Randomized Complete Block Design (RCBD) to control for domain-specific variations by classifying papers into eight academic categories: Mathematics, Computer Science, Physics, Quantitative Biology, Quantitative Finance, Statistics, Economics, and Electrical Engineering & Systems Science. Our goal is to determine whether there are significant differences in extraction and summarization accuracy between the two models and to quantify the effects of academic domain on performance.

## Methods

### Experimental Design
A randomized complete block design (RCBD) was employed to evaluate the performance of the two LLMs (treatments) across eight document categories (blocks). A total of 200 PDF documents were randomly sampled from the arXiv repository for each block, ensuring a balanced representation of domains. The treatments (Gemini 3.5 and Llama 3.1) were applied to each paper in each block in a randomized manner to extract the title and author names from the documents. Meanwhile, the two models were asked to summarize the introduction of each paper. The extracted data were compared against ground truth annotations to determine the count of correct extractions for each treatment-block combination, and the summary data were compared against the abstract by measuring their cosine similarity for each treatment-block combination.

### Data Collection and Preprocessing
Documents were categorized into eight blocks based on their subject area. Ground truth metadata (titles, author names and abstracts) were extracted from the original documents. The LLMs were applied to the documents with the following prompt message for extraction: 

<div style="text-align: center; background-color: #CCFFFF; width: 80%; margin: 0 auto;">
  "Extract the title and author names from this research paper: \"{}\". If a field cannot be found, return 'NA'. Do not alter the original information in any way. Extract it and keep it as original as possible."
</div>

and the prompt message for summarization:

<div style="text-align: center; background-color: #CCFFFF; width: 80%; margin: 0 auto;">
  "Summarize the following content into one paragraph. Do not produce key bullets or lists."
</div>

### Statistical Analysis
Analysis of Variance (ANOVA): A two-way ANOVA was conducted to test for significant differences in the mean cosine similarity of the two treatments across the blocks. This analysis assessed the main effects of the treatments and blocks in summaring information.

Logistic Linear Regression: A logistic regression model was fitted to evaluate the influence of the treatment (LLM type) and block (document category) on the probability of correct extraction. The model included treatment and block as covariates, without interaction terms to explore domain-specific performance variations.

Bayesian Regression with MCMC: To complement the frequentist approach, a Bayesian regression model was implemented using Markov Chain Monte Carlo (MCMC) sampling. This allowed for the estimation of posterior distributions for the model coefficients, providing a probabilistic interpretation of the treatment and block effects. The Bayesian approach offers further advantages over generalized linear models (GLMs) by incorporating prior knowledge and quantifying uncertainty in parameter estimates.

### Software and Tools
All statistical analyses were performed using R with Metropolis-Hastings algorithm for Bayesian regression. Pre-processing and document handling were implemented in Python using the pytesseract and Image libraries.

## Results
The tables below is a subset of the experimental outcomes:

```{r}
summ_results%>%group_by(Model, Primary_Cat) %>%slice_head(n = 1) %>%ungroup()%>%
  mutate(Abstract=str_trunc(Abstract, width = 50, side = "right"),
                      Summary=str_trunc(Summary, width = 30, side = "right"))%>%
  kable(caption = "Table 1: Cosine Similarity Between Model Generated Summary and Abstract",
                     col.names = c("ID", "Model", "Primary Category","Abstract", "Summary","Cosine Similarity"))%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),full_width = FALSE)%>%
  column_spec(4:5, width = "150px")

countdata%>%kable(caption = "Table 2: Counts of Correct Extraction of Titles and Author Names",
                  col.names = c("Model", "Primary Category","Author Names", "Titles","Total"))%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),full_width = FALSE)
```

```{r}
summ_results%>%plot_ly(y=~cosine_sim_Summary,color=~Model, type="box")%>%
  layout(title = "Cosine Similarity between Summary and Abstract by Model",  
         xaxis = list(title = "Model"),                   
         yaxis = list(title = "Cosine Similarity"))        
```

```{r}
selectInput(inputId = "Model",
            label = "Select a model:",
            choices = unique(exp_results$Model), # Populate with distinct values
            selected ="Llama")
plotlyOutput("plot1")

```

```{r}
#| context: server

shinydata<-reactive({
  df<-countdata
  colnames(df)<-c("Model", "Primary_Cat", "Author", "Title", "Total")
  df<-df%>%
    filter(Model == input$Model)%>%
    pivot_longer(Author:Title, names_to = "Type", values_to = "Count")
  return(df)
})

output$plot1 <- renderPlotly({
  data<-shinydata()
  p<-data%>%plot_ly(
    x = ~Primary_Cat, 
    y = ~Count/200, 
    type = "bar",
    color = ~Type, 
    colors = "Set3" 
  )%>%
    layout(
      title = paste("Distribution of Extraction Accuracy By Category for Model:", input$Model),
      xaxis = list(title = "Category"),
      yaxis = list(title = "Accuracy",range = c(0.55, 1)),
      barmode = "group"
    )
  return(p)
})


```

## Discussion

### ANOVA on Difference in Cosine Similarity

The effect model is:
$$
y_{ij} = \mu +\tau_i + \beta_j + \epsilon_{ij}, \quad i=1,2,...,a, \quad j=1,2,...,b
$$
where a is the number of treament levels and b is the number of blocks. In this experiment, a=2 and b=8.

Let $\mu_i=\mu +\tau=\frac{\sum_{j=1}^b (\mu +\tau_i + \beta_j)}{b}$, the hypothesis of interest is:

\begin{align}
H_0: \mu_1=\mu_2 \\
H_1: \mu_1 \neq \mu_2
\end{align}

```{r}
cos_sum_data = summ_results%>%transmute(Model=factor(Model), Primary_Cat=factor(Primary_Cat), cosine_sim_Summary)
fit0=aov(cosine_sim_Summary~Model, data = cos_sum_data)
fit1=aov(cosine_sim_Summary~Model+Primary_Cat, data = cos_sum_data)
fit2=aov(cosine_sim_Summary~Model*Primary_Cat, data = cos_sum_data)

# Capture the printed ANOVA output as a text string
anova_text<-capture.output(print(anova(fit0, fit1, fit2)))
summary_text <- capture.output(print(summary(fit1)))

# Print the ANOVA output inside the styled div
HTML(paste0("<pre class='anova-output'>", paste(anova_text, collapse = "\n"), "</pre>"))

title <-as.character(formula(fit1))
title <- paste(title[2], title[1], title[3])
title <- paste0("<h3 class='anova-title'>Model: ", title, "</h3>")
HTML(paste0("<pre class='anova-output'>", title, paste(summary_text, collapse = "\n"), "</pre>"))
```

Note the decrease in the residual sum of squares, RSS in the additive model. This suggests that blocking can reduce residual errors given the appropriate design. In addition, the general complete block design with replications in each block allows us to examine the significance of the interaction term. The ANOVA table above has shown that we didn't have enough evidence to conclude that the interaction term affects the difference, but the category of the research paper is significant in determining the performance difference.

```{r}
#fit1$model%>%mutate(e=rstandard(fit1))%>%arrange(e)%>%qqplotly()
```

Plot 1 and 3 are plots of residuals and standardized residuals against the fitted values, from which we have observed unequal variances.  

Plot 2 is the Q-Q plot of the standardized residuals, suggesting that linearity and Normality assumptions of the model are reasonable to describe the data.

Plot 4 suggests that the fitted model is sufficient in the sense that we didn't have nonrandom pattern about zero

Plot 5 is a run order of the standardized residuals and it exhibits randomly scattered values around zero with no discernible pattern, indicating independence of the error terms.

```{r, fig.height=18, fig.width=12}
par(mfrow = c(3,2))
plot(fit1)
plot(rstandard(fit1))
```


### GLM
A logistic regression model was fitted to evaluate the influence of the model type and the document cateogy on the success rate of correct extraction of titles and author names from scientific papers. The following model summary suggested that the model type was a major factor 
```{r}
fit3=glm(cbind(CountAuth, 200-CountAuth)~Model+Primary_Cat, family = binomial(link = 'logit'), data=countdata)
summary_text <- capture.output(print(summary(fit3)))
title <-as.character(formula(fit3))
title <- paste(title[2], title[1], title[3])
title <- paste0("<h3 class='anova-title'>Model: ", title, "</h3>")
HTML(paste0("<pre class='anova-output'>", title, paste(summary_text, collapse = "\n"), "</pre>"))
```

Interpretation: Llama model increases the odds of success compared to Gemini by `r paste0(round((exp(fit3$coefficients[2])-1)*100,2), "%")`.

Inedequate model if without blocking:
```{r}
nullmodel=glm(cbind(CountAuth, 200-CountAuth)~Model, family = binomial(link = 'logit'), data=countdata)
anova_text<-capture.output(print(anova(nullmodel,fit3,test="LRT")))

# Print the ANOVA output inside the styled div
HTML(paste0("<pre class='anova-output'>", paste(anova_text, collapse = "\n"), "</pre>"))

```

Overfitting if considering interaction:
```{r}
fullmodel=glm(cbind(CountAuth, 200-CountAuth)~Model*Primary_Cat, family = binomial(link = 'logit'), data=countdata)
anova_text<-capture.output(print(anova(fit3,fullmodel,test="LRT")))

# Print the ANOVA output inside the styled div
HTML(paste0("<pre class='anova-output'>", paste(anova_text, collapse = "\n"), "</pre>"))

```

```{r}
fit3$model%>%mutate(e=residuals(fit3, type = "deviance"))%>%arrange(e)%>%qqplotly()
```

### Bayesian Analysis

Consider a two-sample hypothesis test:
$$
H_0: ~ p_1 \leq p_2 \\
H_1: ~ p_1 > p_2
$$

such that we would like to know if the success probability of model 1 $p_1$ is greater than the one of model 2 $p_2$. We adapted the Bayesian approach to gain a different understanding of our unknown parameter and to draw inference on it. Assuming we have a prior knowledge of this success rate, say 80%, before observing the data. The Baysian approach allows us to "correct" our degree of beliefs based on the actual observation. In particular, the Bayes' Theorem says that 
$$
P(A|B) = \frac{P(A) \cdot P(B|A)}{P(B)}
$$
where $P(A)$ is the prior probability reflecting our prior belief and $P(A|B)$ is the posterior probability after having observed the event B. Such analysis allows us to make an inference on the success rate in a very different point of view.

Suppose we assume $Beta(\alpha, \beta)$ is the prior distribution of the success rate p, we then have a conjugate pair of the prior and posterior distributions. The posterior distribution is $Beta(\alpha+\sum_{i=1}^n x_i, \beta+n-\sum_{i=1}^n x_i)$ with the mean and variance:
$$
\begin{gather*}
E(p|\vec{x})=\frac{\alpha+\sum_{i=1}^n x_i}{\alpha+\beta+n}\\
Var(p|\vec{x})=\frac{(\alpha+\sum_{i=1}^n x_i)(\beta+n-\sum_{i=1}^n x_i)}{(\alpha+\beta+n)^2(\alpha+\beta+n+1)}
\end{gather*}
$$
One can see that asymptotically when we have a large sample size n, the mean of the posterior distribution approaches to the maximum likelihood estimator $\hat{p} =\frac{1}{n}\sum_{i=1}^n x_i$ in the frequentist approach. Thus the specification of a prior is less important if the sample size is large.  This allows us to make a subjective assumption of the success rate based on a probability distribution instead of viewing it as a fixed but unknown number in the frequentist hypothesis test. In the case where $\alpha=\beta=1$, we obtain the non-informative prior $Beta(1,1)$, which is $Uniform([0,1])$ distribution. It's called non-informative because we assume that all p are equally probable. In other words, we no longer put a subjective assumption on $p$. 

After having obtained the posterior distribution, we can get the ($1-\alpha$)-level credible interval for the success rate based on the quantile of the posterior as $[L(\vec{x}), R(\vec{x})]$ such that:
$$
\mathbb{P}(\mathbf{p} \in [L(\vec{x}), R(\vec{x})] |\vec{x}) = \int_{L(\vec{x})}^{R(\vec{x})} f(p|\vec{x})dp = 1-\alpha
$$
With such definition, it is perfectly fine to interpret the credible interval as: the probability that our success rate falls within the given credible interval is (1-$\alpha$)%. 

Various research papers have shown that the Bayesian Uniform prior yields better estimation of the confidence interval than the one obtained from the normal approximation of the binomial distribution. We made a simulation to justify this.

```{r}
# Instantiate

bayes1=Bayes$new(data=countdata, alpha=1, beta=1, alphalevel=0.05)
bayes1$simBayesCoverage()
```

With Bayesian analysis one can evaluate directly the probability of parameter $p_1$ greater than parameter $p_2$ since we have the posterior distributions of the parameters. Therefore with Monte Carlo method, we tested the two-sample hypothesis: $H_0: p1 \leq p2$ VS. $H_1: p1 > p2$ where $p_1$ is the parameter of Model1, and $p_2$ is the parameter of Model2.  We evaluated the following integral:
$$
P(p_1 > p_2 |\vec{x}) = \int_0^1 \int_0^1 \mathbb{I}(p_1 > p_2 |\vec{x}) \cdot f(p_1|\vec{x})f(p_2|\vec{x})dp_1 dp_2 = E[\mathbb{I}(p_1 > p_2 |\vec{x})]
$$
where $\mathbb{I}(p_1 \leq p_2|\vec{x})$ is the indicator function given the data and $f(p_1|\vec{x})$, $f(p_2|\vec{x})$ are the posterior Beta probability density functions for Model1 and Model2 respectively. To estimate this probability, we simulated pairs of Beta random variables from the two posterior distributions simultaneously, and calculate the average of the indicator function. 

### MCMC in Bayesian Inference
Consider generating a sequence of Markov chain $\{\mathbf{X} \}^{(t)}, t=0,1,2...$ whose state space is either discrete or continuous. When the chain is irreducible and aperiodic, it converges in distribution to a long-run stationary distribution. In MCMC, our goal is to construct such a chain for which the stationary distribution equals the target distribution $f$ for sufficient large t. The difficulty to determining such distributional approximation arises because the stationary distribution can be vastly different from $f$ when t is small, and because the chain of random variables has dependency.

One popular and basic algorithm to generate such a chain is the Metropolis-Hastings algorithm:

Step 1: Pick a proposal distribution $g$ and draw the initial starting point $\mathbf{X}^{(0)} = \mathbf{x}^{(0)}$ at time t=0, with the requirement that $f(\mathbf{x}^{(0)})>0$.

Step 2: Given $\mathbf{x}^{(t)}$, we propose a candidate value $\mathbf{x^*}$ from the proposal distribution $g(\mathbf{X^*} | \mathbf{x^{(t)}})$ and we calculate the following acceptance probability:
$$
\alpha(\mathbf{x}^{(t)} , \mathbf{x}^*) = min \left\{ 1,  \frac{f(\mathbf{x}^*)g(\mathbf{x}^{(t)}|\mathbf{x}^*)}{f(\mathbf{x}^{(t)})g(\mathbf{x}^*|\mathbf{x}^{(t)})} \right\}
$$
Step 3: accept the candidate $\mathbf{x^*}$ according to the following:
$$
\mathbf{X}^{(t+1)} = \begin{cases}
\mathbf{x}^* \quad \text{with probability} \quad \alpha\\
\mathbf{x}^{(t)} \quad \text{otherwise}
\end{cases}
$$
We then iterate over large t to generate the chain.

Now suppose we have the proposal distribution $g$ as such: $g(\mathbf{x}^*|\mathbf{x}^{(t)}) = g(\mathbf{x}^*)$, i.e. an independence structure. In this case the acceptance probability simplifies to:

$$
\alpha(\mathbf{x}^{(t)} , \mathbf{x}^*) = min \left\{ 1,  \frac{f(\mathbf{x}^*)g(\mathbf{x}^{(t)})}{f(\mathbf{x}^{(t)})g(\mathbf{x}^*)} \right\}
$$
In Bayesian statistics, suppose we use the prior distribution $p(\mathbf{\theta})$ as the proposal distribution $g$ and the likelihood function $L(\mathbf{\theta|y})$. The posterior distribution $p(\boldsymbol{\theta|y}) \propto p(\boldsymbol{\theta}) L(\boldsymbol{\theta|y})$, so the acceptance ratio becomes:

$$
R(\boldsymbol{\theta}^{(t)}, \boldsymbol{\theta}^*) = 
\frac{p(\boldsymbol{\theta}^* \mid \mathbf{y}) p(\boldsymbol{\theta}^{(t)})}{p(\boldsymbol{\theta}^{(t)} \mid \mathbf{y}) p(\boldsymbol{\theta}^*)} 
= \frac{p(\boldsymbol{\theta}^*) L(\boldsymbol{\theta}^* \mid \mathbf{y}) p(\boldsymbol{\theta}^{(t)})}{p(\boldsymbol{\theta}^{(t)}) L(\boldsymbol{\theta}^{(t)} \mid \mathbf{y}) p(\boldsymbol{\theta}^*)} 
= \frac{L(\boldsymbol{\theta}^* \mid \mathbf{y})}{L(\boldsymbol{\theta}^{(t)} \mid \mathbf{y})}
$$
This gives us an intuitive understanding of the algorithm: if the candidate generated from the prior distribution yields a higher likelihood given the data, then we definitely accept it! Otherwise, maybe.

### Bayesian Linear Regression
Bayesian logistic regression is a probabilistic approach to modeling binary outcomes using logistic regression while incorporating prior knowledge about parameters. Inference can be different to the conventional regression analysis in the frequentist approach. Instead of point estimates, we obtain a distribution over parameters. Statistical uncertainty is quantified using credible intervals rather than p-values. In addition we can compute posterior probabilities, e.g. $P(\boldsymbol{\beta}>0|\mathbf{y})$, to assess variable importance. 

Model fit assessment in Bayesian logistic regression is different from the frequentist approach. Instead of relying on p-values and significance tests, Bayesian analysis focuses on posterior predictive checks. It compares simulated data from the posterior distribution to the observed data to check model fit. We included a simulation to demonstrate this through Bayesian logistic linear regression:

```{r}
sim_model <- BayesianLogisticRegression$new(CountAuth ~ Model + Primary_Cat, countdata)
sim_model$simulation_check()
```

For this experiment we have nine parameters as listed in the logistic regression in the previous section. We proposed the $Normal(\mu, \sigma^2)$ distribution as the prior for the parameters where $\mu$ are given by the estimated coefficients from the previous section and $\sigma^2 = 0.01$. Then we used the Metropolis-Hastings algorithm to obtain their posterior samples. The proposed model for our experiment is:

\begin{gathered}
Y_i | \beta_j \sim Bernoulli(p_i) \quad \text {with} \quad logit(p_i)=\boldsymbol{\beta}\mathbf{X_i} \\
\boldsymbol{\beta} \sim N(\boldsymbol{\mu}, \sigma^2 \mathbf{I})
\end{gathered}

```{r}
n_iter=10000
init_values = as.numeric(coefficients(fit3))
```

```{r, echo=TRUE}

# Instantiate a model object
model <- BayesianLogisticRegression$new(CountAuth ~ Model + Primary_Cat, countdata)

# Run Metropolis-Hastings
posterior_estimates <- model$metropolis_hastings(n_iter = n_iter, init = init_values, proposal_sd = 0.1)
```

```{r}
posterior_estimates%>%kable(col.names = c('Posterior Mean', 'Posterior Standard Deviation','95% Credible Interval'))%>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Interpretation is the same as in GLM: Llama model increases the odds of success compared to Gemini by `r paste0(round((exp(posterior_estimates['Llama',1])-1)*100,2), "%")`.

The following predictive check shows the histogram of the proportion of correctly extracted author names for each model and category combination in each of 10000 posterior simulated data sets. the vertical line represents the observed success rate.

```{r}
model$predictive_check(posterior_samples = model$posterior_samples, data=model$data)
```

# Conclusion
In this study, we evaluated the performance of two large language models (LLMs), Gemini 3.5 and Llama 3.1, for extracting titles and author names from research papers, as well as summarizing paper content. A randomized complete block design (RCBD) was used to assess the models across eight academic domains, and various statistical methods, including ANOVA, logistic regression, and Bayesian regression, were employed to analyze the results. The analysis revealed that the Llama model outperformed Gemini in terms of correct extraction of author names, while scientific domain-specific variations also influenced performance. Bayesian analysis, particularly using MCMC sampling, provided a probabilistic framework to assess the models' success rates and make inferences about the differences in their performances. Predictive checks and posterior simulations further supported the findings, offering insights into the models' ability to replicate observed success rates. These results highlight the effectiveness of Llama in document extraction tasks, especially when considering domain-specific effects and leveraging probabilistic approaches like Bayesian analysis for more robust inference.